---
layout: archive
title: "Eigenvalues and Eigenvectors"
permalink: /notes/linear-algebra/eigen/
author_profile: false
--- 
<hr style="border: 2px solid black;">
Eigenvalues and Eigenvectors show up everywhere in math (and pretty much any quantitative field) and are not only limited to the study of linear algebra. Therefore, an inuitive and rigorous understanding of these concepts is extremely beneficial and a lot of the times necessary. First, it is necessary to think in terms of vectors and linear transformations. We can have vectors of pretty much anything, though it might help to think of normal arrow ended vectors. Vectors are simply elements of a vector space (kind of circular reasoning but a vector space is a set with a defined sort of structure, you can get more detailed notes I have here: ). A matrix is simply a defined transformation of a vector, to another vector. Now, this is when we are considering the appropriate dimensions, as we can multiply a matrix by another matrix and get a matrix as a result instead of a vector. However, we can think of that matrix as a sort of collection of vectors. Regardless, for simplicity we will consider an \\(m\times n\\) matrix multiplied on the left to a vector, i.e. an \\(n\times 1\\) list of elements (if we are considering it as a column vector, we could just say it's \\(n\times 1\\) for a row vector). The result will indeed be an \\(m\times 1\\) vector, however, this vector could technically be in another space if \\(m \neq n\\), so for now let \\(m=n\\). So now that we have a matrix that is a linear transformation of one vector in our space to another in the same space, we might then ask, what did the matrix actually do to our vector? For simplicity, our matrices are non-singular and therefore all determinents are 0.

## Motivation
One part of the previous question, is to consider if our vector was stretched or compressed. Intuitively, the vector was stretched if the norm of the new transformed vector is greater than the norm of the original vector and compressed otherwise (assume non-zero norms). While this is true, this does not generalize to an arbitrary vector. We want to consider properties of the matrix, and therefore we need a more general notion of what a matrix might do to an arbitrary vector. A matrix can transform different vectors in different ways, so this method wouldn't tell us something intrinsic about the matrix, rather just how the matrix interacts with that specific vector.
