<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="robots" content="noindex, nofollow">

  <title>Personal PDE System</title>
  <link rel="stylesheet" href="../styles.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['\\[', '\\]']],
        processEscapes: true,
        tags: 'none'
      }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <header class="site">
    <a href="../../index.html" class="back">← Back</a>
    <h1 class="title">Personal System of PDEs (Reaction-Diffusion)</h1>
  </header>

  <main>
    <p>
      I wanted to extend the concept of the Fisher-KPP, which is probably the most basic reaction-diffusion equation. 
      It can model the behavior of concentration spread and growth. However, this system is monostable, meaning any non-zero initial 
      condition will eventually grow to its carrying capacity everywhere. This behavior isn't particularly interesting, 
      I was looking to capture a cyclic relationship between producers and consumers. By making some assumptions on their 
      behaviors, I arrived at a minimal model which exhibits a range of unique behaviors.
    <h2>PDE Derivation</h2>
    <p>
        First, I began with the Fisher-KPP.
    </p>
    <div class="math-block">\[
      \frac{\partial u}{\partial t} = D_u\Delta u + r(1-u)
    \]</div>
    <p>\(u\) represents a concentration of something, and therefore is naturally 
        bounded between \(0\) and \(1\). The PDE also enforces this boundary as the 
        the spatially homogenous ODE is positive at \(0\) and \(0\) at \(1\). Furthermore, diffusion 
        alone cannot break this limit. As mentioned, this captures two main behaviors.
        <ol>
            <li>Diffusion: \(u\) will naturally move from areas of higher concentration to lower concentration.</li>
            <li>Reaction: The term, \(ru(1-u)\), is known as logistic growth. The solution to the spatially homogenous system is simply the solution to the 
                logistic equation, which is a sigmoid function. This means that any non-zero population grows according to the sigmoid function to its carrying capacity (\(1\)).
            </li>
        </ol>
        This can be interpreted as follows. For some non-zero initial concentration of \(u\), 
        it will spread out in space, lowering the concentration of its initial position. However, once a non-zero 
        concentration touches a point in space that originally had \(0\) concentration, the population will begin 
        to start growing logistically (according to the reaction term). However, diffusion will still drain it some, if 
        it has a higher concentration than its neighbors. The logistic term usually overpowers the diffusive term pretty quickly, and 
        we can observe traveling waves of varying concentration densities.
    </p>
    <p>
        Building on this, I wanted a similar behavior but with more delicate reactions. Depending on the nature of the phenomenon you are 
        trying to capture, different considerations and assumptions must be placed, even on the same phenomenon. In my case, 
        I wanted to capture the cyclic relationship of a population that depends on another population for survival. However, in depending on that 
        other population, they also deplete it. You can think of this as a wolf population living on deer. They naturally lower the deer population to survive, however, 
        killing all the deer would cause the wolf population to collapse. Many models of this type already exist, most notably the family of 
        Lotka-Volterra equations with diffusion. However, the underlying ODE produces closed orbits, meaning any initial population will 
        follow their own closed orbit. I was looking to have a more interesting prey/producer field, and wanted it to have a few features.
    </p>
    <p>
        To clarify, in this new model, \(u\) represents the concentration of the predator/consumer population (field), and 
        \(v\) represents the concentration of the prey/producer population (field).
        I wanted the environmental field to exhibit these characteristics:
        <ol>
            <li>Slow recovery in time, independent of the predator/consumer population. This is captured by \(v_t = \gamma(1-v)\). This term 
                essentially says that the population will increase more when the population is low, and increase less when the population is high. 
                In other words, we see diminishing growth as this population occupies its carrying capacity. 
            </li>
            <li>Death of the population (\(v\)) due to the predator population (\(u\)). A common 
                way to capture this behavior is through the coupling term: \(-\beta u v\). This is consistent with 
                the depletion term in the Lotka-Volterra equations.
            </li>
            <li>Growth surge once a certain threshold is reached. This is the most delicate part of the 
                equation. However, I decided to capture this through a cubic term: \(\alpha v(1-v)(v-v_c)\). 
                Where \(\alpha\) is a constant for the rate of growth. \(v_c\) is the threshold that determines when the 
                cubic growth kicks in. This cubic function has roots at \(0\), \(v_c\), and \(1\). It's negative on the interval
                \((0, v_c)\), meaning it actually captures that the population dies out if it doesn't reach a "critical mass". 
                This can be thought of as populations that need other members of that population for survival. A counter example would be 
                bears, which typically live alone. We can observe this behavior in herd animals, that are easily picked off if they tried living 
                apart from their population. However, there is a very important balance going on now, which drives the behavior of the PDE. 
                Although the cubic term will essentially deplete low populations, the linear growth term: \(\gamma(1-v)\) naturally 
                restores the population. We can think of this as either a sufficient population that leaks into our area, 
                and therefore providing a consistent replenishment/restoration of the population, or we could consider it like plants that will naturally 
                repopulate. This makes \(\gamma\), \(v_c\), and \(\alpha\) extremely important the behavior of the system. 
                For different parameters, we can get complete population death, cyclic populations, or total stability of both populations. 
                With the parameters I typically use, the linear term is strong enough to get the cubic term to its threshold \(v_c\), but it does so slowly,
                which was one of the original goals. 
            </li>
            <li>Population spread. This is easily captured via the diffusion term: \(D_v\Delta v\).</li>
        </ol>
    </p>
    <p>Putting this all together, our PDE for the prey/producer field is given by</p>
    <div class="math-block">\[
      \frac{\partial v}{\partial t} = D_v\Delta v + \alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta uv
    \]</div>

    <p>The consumer field is slightly less complicated. For this, I wanted the following biological characteristics:
        <ol>
            <li>Growth, but regulated by the producer/prey population. Basically, the consumer 
                should grow if population of prey is present, however, we would expect to see slower growth with 
                less available prey/product. I used a standard logistic growth rate, and then a 
                threshold activated sigmoid to regulate the rate of this logistic growth. This term is then given by,
                \(\frac{ru(1-u)}{1+e^{-k_1(v-v_c)}}\). Where \(r\) is the natural growth rate, \(k_1\) is the steepness of the sigmoid, 
                and \(v_c\) is as before, the threshold for the prey/producer growth boom. The denominator is very large (for my parameters around \(122\)) when \(v=0\). This basically allows the population to grow, but 
                it does so very slowly. When \(v=0.4\), the denominator is only \(2\), meaning half the logistic rate. Then as \(v\) grows to \(1\), so does the rate at which \(u\) can. 
            </li>
            <li>Natural death, but regulated by the producer/prey population. Again, we use the idea of a threshold activated sigmoid. 
                The idea here is that not only would we expect to see lower growth when the producer/prey population is low, but we would also expect to see more death, especially if the \(u\) population is 
                already high. This is captured by the term: \(\frac{-c u}{1+e^{-k_2(v_k-v)}}\). Note that the sigmoid threshold here is 
                opposite to the earlier one. This denominator is around \(1\) when the prey population is low, and gets very large as \(v\to 1\). For my parameters, it is around \(122\) at \(v=1\). 
                This basically scales the death rate down by a lot when the prey/producer is sufficient. 
                Furthermore, the \(u\) in this term also regulates the death dependency on its own population. A low \(u\) and \(v\) will basically be zero, but as 
                \(u\) grows, the need for more \(v\) increases and this is captured by that term. If the 
                \(u\) population is at \(1\) but \(v=0\), then this term is at its max strength. This is expected and captures that 
                the population will die most rapidly when there are no resources with a high population. Finally, \(v_k\) is the threshold for the death sigmoid. 
                Meaning it's the point at which the population starts to die off rapidly (if \(u\) dips below this threshold). 
            </li>
            <li>Diffusion, this again just captures the idea that the populations tend to spread out over time. Captured by the term 
                \(D_u\Delta u\)
            </li>
        </ol>
    </p>
        <p>Putting this all together, our PDE for the consumer/predator field is given by</p>
        <div class="math-block">\[
      \frac{\partial u}{\partial t} = D_u\Delta u + 
      \frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} - \frac{cu}{1+e^{-k_2(v_k-v)}} 
    \]</div>
    <p>Therefore, our final system is</p>
    <div class="math-block">\[
        \begin{align}
      \frac{\partial u}{\partial t} &= D_u\Delta u + 
      \frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} - \frac{cu}{1+e^{-k_2(v_k-v)}} \\
      \\
      \frac{\partial v}{\partial t} &= D_v\Delta v + \alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta uv
      \end{align}
    \]</div>


    <h2>ODE Analysis</h2>
    <p>To analyze the behavior of this system, we start by analyzing the spatially homogenous 
        version. In other words, we see how this system would behave if there was no diffusion. 
        This reduces the system to 2 ODEs, which we can then analyze using planar ODE techniques.
    </p>
    <p>Spatially homogenous system:</p>
    <div class="math-block">\[
        \begin{align}
      \frac{d u}{d t} &=
      \frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} - \frac{cu}{1+e^{-k_2(v_k-v)}} \\
      \\
      \frac{d v}{d t} &=  \alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta uv
      \end{align}
    \]</div>
    <h3>Fixed Point Analysis</h3>
    Before we begin our investigation, I will provide the parameters I used initially. 
        <div class="math-block">\[
        \begin{align}
            r &=10\\
            c &= 4\\
            k_1 &= 12\\
            k_2&= 12\\
            v_c &= 0.4\\
            v_k &= 0.55\\
            \alpha &= 12 \\
            \gamma &= 0.6\\
            \beta &= 6
      \end{align}
    \]</div>
    <p>Starting with the first equation, fixed points occur when \(\frac{du}{dt} = 0\)
        Therefore, we seek to find when the following equation holds
    </p>
    <div class="math-block">\[
      \frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} - \frac{cu}{1+e^{-k_2(v_k-v)}} = 0\\
    \]</div>
    <p>Solving, we have</p>
    <div class="math-block">\[
      \frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} = \frac{cu}{1+e^{-k_2(v_k-v)}}\\
    \]</div>
    <p>First, \(u=0\) solve this equation, so that is our first candidate. Now with \(u\neq0\), we can 
        safely divide both sides by \(u\).
    </p>
    <div class="math-block">\[
      \frac{r(1-u)}{1+e^{-k_1(v-v_c)}} = \frac{c}{1+e^{-k_2(v_k-v)}}\\
    \]</div>
    <p>Separating the \(u\) and \(v\)</p>
        <div class="math-block">\[
      r(1-u) = c\frac{1+e^{-k_1(v-v_c)}}{1+e^{-k_2(v_k-v)}}\\
    \]</div>
    <p>Finally,</p>
    <div class="math-block">\[
      u = 1- \frac{c}{r}\cdot\frac{1+e^{-k_1(v-v_c)}}{1+e^{-k_2(v_k-v)}}\\
    \]</div>
    <p>This is another candidate.</p>

    <p>Now for \(v\), we have</p>
    <div class="math-block">\[
      \alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta uv = 0
    \]</div>
    <p>If \(u=0\), then</p>
    <div class="math-block">\[
      \alpha v(1-v)(v-v_c) + \gamma(1-v) = 0
    \]</div>
    <p>It's clear that \(v=1\) satisfies this, so \((0, 1)\) is a fixed point.</p>
    <p>If \(u=0\) and \(v\neq 1\), then</p>
    <div class="math-block">\[
        \begin{align}
        & \alpha v(1-v)(v-v_c) + \gamma(1-v) = 0\\
        &\alpha v(v-v_c)+\gamma = 0\\
        & v^2-v\cdot v_c + \frac{\gamma}{\alpha} = 0
      \end{align}
    \]</div>
    By the quadratic formula, the roots are given by,
    <div class="math-block">\[
        \frac{v_c \pm \sqrt{v_c^2 - 4\frac{\gamma}{\alpha}}}{2}
    \]</div>
    <p>With our parameters, \(v_c^2 = 0.16, 4\frac{\gamma}{\alpha} = 0.2\). Since 
        \(v_c^2 < 0.2\), the discriminant is negative and therefore no real roots to this quadratic exist. 
        Therefore the only fixed point for \(u=0\) is \(v=1\).
    </p>
    <p>Now we check for the other possibility that</p>
        <div class="math-block">\[
      u = 1- \frac{c}{r}\cdot\frac{1+e^{-k_1(v-v_c)}}{1+e^{-k_2(v_k-v)}}\\
    \]</div>
    <p>Plugging this into the equation for \(\frac{dv}{dt}\),</p>
    <div class="math-block">\[
      \alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta v\left(1- \frac{c}{r}\cdot\frac{1+e^{-k_1(v-v_c)}}{1+e^{-k_2(v_k-v)}}\right) = 0\\
    \]</div>
    <p>For simplicity, we will denote</p>
    <div class="math-block">\[
        \begin{align}
      G_1(v) &= \frac{1}{1+e^{-k_1(v-v_c)}}\\
      \\
      G_2(v) &= \frac{1}{1+e^{-k_2(v_k-v)}}
      \end{align}
    \]</div>
    <p>Then our equation becomes</p>
    <div class="math-block">\[
      \alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta v\left(1- \frac{c}{r}\cdot \frac{G_2(v)}{G_1(v)}\right) = 0\\
        \]
        </div>
      <p>This is an implicit equation, and so we can numerically approximate the root. 1 root exists on \([0,1]\),
        \(v \approx 0.370777994\). Plugging this back into the equaiton for \(u\) yields, \(u \approx 0.13293\), and 
        therefore our second fixed point is approximately \((0.13293, 0.370777994)\).
      </p>

      <h3>Stability Analysis</h3>
      <p>To analyze the stability of these points, we will use standard Jacobian stability analysis.
        First, I will prove the interval \([0,1]\) is a trapping region for both \(u\) and \(v\).
      </p>

      <div class="claim-box">
        <div class="claim-title">Claim:</div>
        <p>The region \([0,1]^2\) is forward-invariant for the spatially homogeneous system</p>
          <div class="math-block">\[
            \begin{align}
              \frac{d u}{d t} &=
              \frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} - \frac{cu}{1+e^{-k_2(v_k-v)}} \\
              \\
              \frac{d v}{d t} &=  \alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta uv
              \end{align}
            \]</div>

        <div class="proof-title">Proof.</div>
        <p>
          At \(u=0\) we have \(\frac{du}{dt} \ge 0\) since both reaction terms vanish.
          At \(u=1\) the logistic factor \(1-u\) vanishes leaving \(-\frac{c}{1+e^{-k_2(v_k-v)}}\), which is always negative 
          with positive constant \(c\). So the function is decreasing at \(u=1\). At \(v=1\), the cubic term 
          and linear terms vanish leaving \(-\beta u\), which is always negative for \(\beta > 0\), since 
          \(0 \leq u \leq 1\). Finally, at \(v=0\), we are left with \(\frac{dv}{dt} =\gamma\), which is positive when 
          \(\gamma > 0\). Therefore, the vector field points inward on all sides of the square \([0,1]^2\).
        </p>
      </div>
        <p>
          Now that we know this region of \(\mathbb{R}^2\) is bounded, we must treat the fixed point 
          \((0,1)\) carefully, since it lies on the boundary. We can bypass stability analysis via the 
          Jacobian here, since we can directly analyze the vector field nearby. Consider 
          \((u, v) = (\epsilon, 1)\) for some small \(\epsilon > 0\) (a point very close to the fixed point 
          on the boundary). For small \(u\), we have \(1 - u \approx 1\), so \(ru(1 - u) \approx ru\). 
          Further, \(1 + e^{-k_1(1 - v_c)} = 1 + e^{-12(1 - v_c)} = 1 + e^{-7.2} \approx 1\), so the 
          denominator in the growth term is essentially \(1\). For the death term, 
          \(1 + e^{-k_2(v_k - 1)} = 1 + e^{5.4} \approx 222\), which makes the entire death term negligible. 
          Therefore the vector field is approximately \(\frac{du}{dt} \approx ru\), which is positive for 
          \(r > 0\). Any small positive perturbation in \(u\) grows, so the fixed point \((0,1)\) is unstable.
        </p>

        <p>Now for the fixed point \((0.13293, 0.370777994)\), since it lies in the interior of
          this forward-invariant region, we can safely use the Jacobian for stability analysis. First, let 
          \(\frac{du}{dt} = F(u,v)\) and \(\frac{dv}{dt} = G(u,v)\).
          Recall the Jacobian matrix is given by,
        </p>
          <div class="math-block">\[
              J = 
            \begin{bmatrix}
              \frac{\partial F}{\partial u} & \frac{\partial F}{\partial v}\\
              \\
              \frac{\partial G}{\partial u} & \frac{\partial G}{\partial v}\\
              \end{bmatrix}
            \]</div>
          <p>Computing this term by term, </p>
              <div class="math-block">\[
              \begin{align}
              J_{1,1} &= 
              \frac{\partial F}{\partial u} = \frac{\partial}{\partial u}\left(\frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} - \frac{cu}{1+e^{-k_2(v_k-v)}}\right)\\
              &= \frac{r(1-2u)}{1+e^{-k_1(v-v_c)}} - \frac{c}{1+e^{-k_2(v_k-v)}}\\
              \end{align}
            \]</div>
            <div class="math-block">\[
              \begin{align}
              J_{1,2} &= 
              \frac{\partial F}{\partial v} = \frac{\partial}{\partial v}\left(\frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} - \frac{cu}{1+e^{-k_2(v_k-v)}}\right)\\
              &= ru(1-u)\frac{\partial }{\partial v}\left(\frac{1}{1+e^{-k_1(v-v_c)}}\right) - cu\frac{\partial }{\partial v}\left(\frac{1}{1+e^{-k_2(v_k-v)}}\right)\\
              &= -ru(1-u)\frac{\frac{\partial }{\partial v}\left(1+e^{-k_1(v-v_c)}\right)}{(1+e^{-k_1(v-v_c)})^2} + cu\frac{\frac{\partial}{\partial v}\left(1+e^{-k_2(v_k-v)} \right)}{(1+e^{-k_2(v_k-v)})^2}\\
              &= ru(1-u)\frac{k_1e^{-k_1(v-v_c)}}{(1+e^{-k_1(v-v_c)})^2} + cu\frac{k_2e^{-k_2(v_k-v)}}{(1+e^{-k_2(v_k-v)})^2}\\
              \end{align}
            \]</div>
            <div class="math-block">\[
              \begin{align}
              J_{2,1} &= 
              \frac{\partial G}{\partial u} = \frac{\partial}{\partial u}\left(\alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta uv\right)\\
              &= -\beta v\\
              \end{align}
            \]</div>
          <div class="math-block">\[
              \begin{align}
              J_{2,2} &= 
              \frac{\partial G}{\partial v} = \frac{\partial}{\partial v}\left(\alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta uv\right)\\
              &= (\alpha- 2\alpha v)(v-v_c)+(\alpha v - \alpha v^2) -\beta u - \gamma\\
              \end{align}
            \]</div>

            <p>Then the Jacobian is,</p>
            <div class="math-block">\[J =
              \begin{bmatrix}
              \frac{r(1-2u)}{1+e^{-k_1(v-v_c)}} - \frac{c}{1+e^{-k_2(v_k-v)}} & ru(1-u)\frac{k_1e^{-k_1(v-v_c)}}{(1+e^{-k_1(v-v_c)})^2} + cu\frac{k_2e^{-k_2(v_k-v)}}{(1+e^{-k_2(v_k-v)})^2}\\\
              \\
              -\beta v & (\alpha- 2\alpha v)(v-v_c)+(\alpha v - \alpha v^2) -\beta u - \gamma\\
              \end{bmatrix}
            \]</div>

            <p>The associated characteristic polynomial is, </p>
            <div class="math-block">\[
              \lambda^2 - \text{tr}(J)\lambda + \det(J) = 0 \\
              \\ 
              \implies \lambda^2 - \lambda(J_{1,1} + J_{2,2}) + (J_{1,1}J_{2,2} - J_{1,2}J_{2,1}) = 0
            \]</div>
            <p>This is a quadratic in \(\lambda\) therefore, we can use quadratic formula</p>
            <div class="math-block">\[
              \lambda_{1,2} = \frac{-\text{tr}(J) \pm \sqrt{\text{tr}(J)^2 - 4\det(J)}}{2}
            \]</div>
            <p>The Jacobian evaluate to: </p>
              <div class="math-block">\[
              \begin{bmatrix}
                -0.549506 & 3.9495\\
                -2.224667964 & 1.311357856
              \end{bmatrix}
            \]</div>
            <p>Plugging this into the formula</p>
            <div class="math-block">\[
              \lambda_{1,2} = \frac{0.76185 \pm \sqrt{-33.844081}}{2} = \frac{0.76185 \pm 5.87i}{2} = 0.38 \pm 2.835 i
            \]</div>
            <p>Since the real part is positive, and the roots are complex, we have an unstable focus/spiral.</p>

            <p>Now we claim the existence of a limit cycle via the Poincaré–Bendixson theorem.</p>

            <div class="claim-box">
              <div class="claim-title">Claim:</div>
              <p>The system admits at least one limit cycle.</p>

              <div class="math-block">\[
                \begin{aligned}
                  \frac{du}{dt} &= \frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} - \frac{cu}{1+e^{-k_2(v_k-v)}} \\
                  \\
                  \frac{dv}{dt} &= \alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta uv
                \end{aligned}
              \]</div>

              <div class="proof-title">Proof.</div>
              <p>
                We have shown that the square \([0,1]^2\) is compact and positively invariant for the flow.
                The system has two equilibria in this region: the boundary equilibrium \((0,1)\) and the interior
                equilibrium \((0.13293,\,0.37078)\). The boundary equilibrium is unstable. The interior equilibrium
                has a Jacobian with complex eigenvalues whose real part is positive, so it is an unstable spiral.
              </p>

              <p>
                Because the interior equilibrium is repelling, trajectories starting near it are pushed away from the
                equilibrium instead of converging to it. Since \([0,1]^2\) is positively invariant, these trajectories
                cannot leave the square. The boundary equilibrium is also unstable, so no interior trajectory can
                converge to it.
              </p>

              <p>
                Therefore any trajectory beginning in the interior remains in a compact, positively invariant region
                that contains no attracting fixed points. Its omega-limit set is nonempty, compact, and contains no equilibria. By the Poincaré–Bendixson theorem, this limit set is a
                periodic orbit. Therefore, the system possesses a limit cycle.
              </p>
            </div>
          <p>I have this planar ODE system simulated on my <a href=https://nasser-mohammed.github.io/simulations/programs/Live%20Phase%20Portrait/index.html>live phase portrait simulation page</a>, named 
          "ODE I derived". Below is a simplified demo of the limit cycle.</p>
          <h3>Interactive Phase Portrait of this System</h3>
          <p>Experiment with the system by clicking inside the square. Adjust γ and β to see how the dynamics change.</p>

          <div style="max-width:500px; margin:0 auto 20px;">

            <label for="gammaSlider">γ (environment recovery): 
                <span id="gammaValue">0.6</span>
                  </label>
                  <input type="range" id="gammaSlider" min="0.1" max="2" step="0.01" value="0.6" style="width:100%;">

                  <label for="betaSlider" style="margin-top:10px; display:block;">
                    β (consumer pressure): 
                    <span id="betaValue">6</span>
                  </label>
          <input type="range" id="betaSlider" min="1" max="12" step="0.1" value="6" style="width:100%;">
              <label for="alphaSlider">α (cubic growth rate): 
                <span id="alphaValue">12</span>
                  </label>
                  <input type="range" id="alphaSlider" min="0.1" max="30" step="0.1" value="12" style="width:100%;">

                  <button id="resetBtn" class="reset-button" style="margin-right: 15px;">Reset Trajectories</button>
                  <button id="resetParamsBtn" class="reset-button" style="background:#8d7af7;">Reset Parameters</button>
          </div>

          <canvas id="phaseCanvas" width="500" height="500"
                  style="border:1px solid #333; border-radius:8px; display:block; margin:18px auto;">
          </canvas>

      <h3>Stability Analysis for Different Parameters</h3>
      <p>I will run through another quick stability analysis for a different set of parameters. I choose 
        these parameters as they produce Turing-like patterns, and therefore I will attempt to show whether 
        they are actually Turing patterns in later sections. We already have the Jacobian derived, we just 
        need to find the fixed points again and run them back through this system
      </p>
      <h4>Fixed Points</h4>
      <p>The new set of parameters to consider are</p>
      <div class="math-block">\[
        \begin{align}
            r &=10\\
            c &= 4\\
            k_1 &= 12\\
            k_2&= 12\\
            v_c &= 0.4\\
            v_k &= 0.55\\
            \alpha &= 20 \\
            \gamma &= 0.8\\
            \beta &= 3.6
      \end{align}
    \]</div>
    <p>Repeating earlier arguments, we first find</p>
      <div class="math-block">\[
        \begin{align}
      & \frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} - \frac{cu}{1+e^{-k_2(v_k-v)}} = 0\\
      \\
      & \frac{ru(1-u)}{1+e^{-k_1(v-v_c)}} = \frac{cu}{1+e^{-k_2(v_k-v)}}

      \end{align}
    \]</div>
    <p>Again, \(u=0\) solves this. Now if \(u\neq 0\), divide \(u\)</p>
      <div class="math-block">\[
        \begin{align}
      & \frac{r(1-u)}{1+e^{-k_1(v-v_c)}} = \frac{c}{1+e^{-k_2(v_k-v)}}\\
      \\
      & u = 1- \frac{c}{r}\cdot\frac{1+e^{-k_1(v-v_c)}}{1+e^{-k_2(v_k-v)}}\\

      \end{align}
    \]</div>
    <p>For \(v\), we have that \((0,1)\) is a fixed point regardless of parameters. Now for 
      \(u=0\) and \(v\neq 1\), we had the other possible values as the result of</p>
        <div class="math-block">\[
        \frac{v_c \pm \sqrt{v_c^2 - 4\frac{\gamma}{\alpha}}}{2}
    \]</div>
            <p>Plugging in our new parameters,</p>
      <div class="math-block">\[
        \frac{v_c \pm \sqrt{0.16 - 4\frac{0.8}{20}}}{2} = \frac{0.4 \pm \sqrt{0.16 - 0.16}}{2} = \frac{0.4}{2} = 0.2
    \]</div>
    <p>So we have that \((0, 0.2)\) is a fixed point. Finally, if 
      \(u = 1- \frac{c}{r}\cdot\frac{1+e^{-k_1(v-v_c)}}{1+e^{-k_2(v_k-v)}}\), then plugging this into the
      equation for \(v\) yields</p>
    </p>
      <div class="math-block">\[
      \alpha v(1-v)(v-v_c) + \gamma(1-v) - \beta v\left(1- \frac{c}{r}\cdot \frac{G_2(v)}{G_1(v)}\right) = 0\\
        \]
        </div>
      <p>We cannot analytically find the roots, but numerically we get that \(v\approx 0.4062134\), this makes 
        \(u \approx 0.34531\). Therefore our third fixed point is approximately 
        \((0.34531, 0.4062134)\).
      </p>

      <h4>Stability Analysis</h4>
      <p>We already have our Jacobian derived, we just need to numericall compute it.</p>
        <div class="math-block">\[
              \begin{bmatrix}
              \frac{r(1-2u)}{1+e^{-k_1(v-v_c)}} - \frac{c}{1+e^{-k_2(v_k-v)}} & ru(1-u)\frac{k_1e^{-k_1(v-v_c)}}{(1+e^{-k_1(v-v_c)})^2} + cu\frac{k_2e^{-k_2(v_k-v)}}{(1+e^{-k_2(v_k-v)})^2}\\\
              \\
              -\beta v & (\alpha- 2\alpha v)(v-v_c)+(\alpha v - \alpha v^2) -\beta u - \gamma\\
              \end{bmatrix}
          \]</div>
      <p>Evaluating at \((0,1)\), we have</p>
        <div class="math-block">\[
              \begin{bmatrix}
              9.974 & 0\\
              \\
              -0.8  & -12.8\
              \end{bmatrix}
          \]</div>
      <p>For the point \((0, 0.2)\), the Jacobian is</p>
      <div class="math-block">\[
              \begin{bmatrix}
              -3.109176908 & 0\\\
              \\
              -0.72 & 0\\
              \end{bmatrix}
          \]</div>
      <p>For the last point, \((0.34531, 0.4062134)\), the Jacobian is</p>
        <div class="math-block">\[
              \begin{bmatrix}
              -1.790770106 & 8.899317969\\\
              \\
              -1.462368 & 2.80427482\\
              \end{bmatrix}
          \]</div>

      <p>Recall the characteristic polynomial,</p>
      <div class="math-block">\[
              \lambda^2 - \text{tr}(J)\lambda + \det(J) = 0 \\
              \\ 
              \implies \lambda^2 - \lambda(J_{1,1} + J_{2,2}) + (J_{1,1}J_{2,2} - J_{1,2}J_{2,1}) = 0
            \]</div>
      <p>Evaluating for \((0,1)\), we have</p>
      <div class="math-block">\[
        \lambda^2 + 2.8256\lambda - 127.6672 = 0
      \]</div>
      <p>Using quadratic formula,</p>
      <div class="math-block">\[
        \begin{align}
        \lambda &= \frac{-2.8256 \pm \sqrt{2.8256^2 + 4\cdot127.6672}}{2} = \frac{-2.8256 \pm 22.77399833}{2}\\
        \\
        &\implies \lambda_1 = 9.974199165, \ \ \lambda_2 = -12.799799165
        \end{align}
      \]</div>
      <p>Since one eigenvalue is positive and one is negative, we have that \((0,1)\) is a <strong>hyperbolic saddle.</strong></p>

      <p>For \((0. 0.2)\), we plug these values into the characteristic polynomial</p>
        <div class="math-block">\[
        \begin{align}
        \lambda &= \frac{-3.1091769 \pm \sqrt{3.1091769^2 + 4\cdot0}}{2}\\
        \\
         &= \frac{-3.1091768 \pm \sqrt{3.1091769^2}}{2} =  \frac{-3.1091768 \pm 3.1091768}{2} \\
        \\
        &\implies \lambda_1 = 0, \ \ \lambda_2 = -3.1091769
        \end{align}
      \]</div>
      <p>Since one eigenvalue is zero and the other is negative, we have that \((0, 0.2)\) is a 
        <strong>non-hyperbolic fixed point</strong>, meaning the Jacobian cannot determine stability alone.
        It has a center manifold, which we will find later to determine the nature of this fixed point. For
        now I will continue with the Jacobian for the last point.</p>

      <p>For \((0.34531, 0.4062134)\), we plug the values into the characteristic polynomial again</p>
        <div class="math-block">\[
        \begin{align}
        \lambda &= \frac{1.013504714 \pm \sqrt{1.027191805 - 31.96906521}}{2} \\
        &= \frac{1.013504714 \pm \sqrt{-30.9418734}}{2} =  -0.506752357 \pm 2.781270995 \ i\\
        \\
        &\implies \lambda_1 = 0.506752357 + 2.781270995 \ i, \\ &\implies \lambda_2 = 0.506752357 - 2.781270995 \ i
        \end{align}
      \]</div>
      <p>So for the final fixed point, we have a <strong>unstable (repelling) spiral/focus</strong>. Trajectories near it will 
        spiral away from it over time.
      </p>
      <p>By earlier logic, the saddle on the boundary repels trajectories inside the unit square. Further, the unstable spiral at \((0.34531, 0.4062134)\)
        pushes trajectories away. A limit cycle does exist, but to prove it we must rule out the potential 
        for the fixed point \((0, 0.20)\) to be a global attractor. Once that is established, then we can 
        use the Poincaré–Bendixson theorem again to prove this limit cycle exists. I briefly mentioned the center manifold, which is a special invariant surface that 
        can help us understand the stability of this fixed point (since linearization fails here). 
        Once we show that this manifold is not attracting, then we will automatically have our limit cycle.
      </p>
      <p>This concludes the Jacobian based stability analysis of the underlying ODEs.</p>

      <h3>Center Manifold Analysis</h3>
      <p>
        At the parameter set producing the Turing-like patterns, we identified a non-hyperbolic fixed point at
        \((u^*, v^*) = (0, 0.2)\). The Jacobian at this point was found to be:
      </p>
      <div class="math-block">
        \[
          J = \begin{bmatrix} -3.109 & 0 \\ -0.72 & 0 \end{bmatrix}
        \]
      </div>
      <p>
        The eigenvalues are \(\lambda_s = -3.109\) (stable) and \(\lambda_c = 0\) (center).
        Because of the zero eigenvalue, the Hartman-Grobman theorem fails, and linearization is insufficient to determine stability.
        We must apply <strong>Center Manifold Theory</strong> to reduce the system to its essential dynamics.
      </p>

      <h4>1. Coordinate Transformation</h4>
      <p>
        First, we shift the equilibrium to the origin. Let:
        \[ \tilde{u} = u, \quad \tilde{v} = v - 0.2 \]
        The eigenvector associated with the stable eigenvalue \(\lambda_s\) is \(\mathbf{w}_s \approx [4.31, 1]^T\), and the eigenvector
        associated with the center eigenvalue \(\lambda_c\) is \(\mathbf{w}_c = [0, 1]^T\).
      </p>
      <p>
        Since \(\mathbf{w}_c\) lies exactly on the \(\tilde{v}\)-axis (and the matrix is lower triangular),
        the center manifold is tangent to the line \(\tilde{u} = 0\). The dynamics of \(\tilde{u}\) are fast (decaying at rate \(e^{-3.1t}\)),
        while the dynamics of \(\tilde{v}\) are slow. By the <strong>Slaving Principle</strong>, the system quickly settles onto the center manifold.
        Since the coupling term \(J_{1,2}\) (top right of Jacobian) is exactly zero, and \(u=0\) is an invariant manifold for the
        first equation, the center manifold is simply \(u = 0\) exactly.
      </p>

      <h4>2. Reduced Dynamics</h4>
      <p>
        We substitute \(u=0\) into the equation for \(\dot{v}\) to find the flow on the center manifold.
        Recall the \(v\)-nullcline equation for \(u=0\):
        \[ f(v) = \alpha v(1-v)(v-v_c) + \gamma(1-v) \]
        We expand this function as a Taylor series around the fixed point \(v^* = 0.2\).
        We know that \(f(0.2) = 0\) and \(f'(0.2) = 0\).
        Therefore, the leading order term is the second derivative (quadratic).
      </p>
      <div class="math-block">
        \[ \dot{\tilde{v}} \approx \frac{1}{2} f''(0.2) \tilde{v}^2 \]
      </div>
      <p>
        Computing the second derivative of the producer dynamics:
        \[ f(v) = -\alpha v^3 + \alpha(1+v_c)v^2 - \alpha v_c v + \gamma(1-v) \]
        \[ f'(v) = -3\alpha v^2 + 2\alpha(1+v_c)v - (\alpha v_c + \gamma) \]
        \[ f''(v) = -6\alpha v + 2\alpha(1+v_c) \]
        Substituting \(\alpha = 20, v = 0.2, v_c = 0.4\):
        \[ f''(0.2) = -120(0.2) + 40(1.4) = -24 + 56 = +32 \]
      </p>

      <h4>3. Stability Conclusion</h4>
      <p>
        The reduced equation on the center manifold is approximately:
        \[ \dot{\tilde{v}} = 16 \tilde{v}^2 \]
      </p>
      <p>
        This represents a <strong>semi-stable</strong> fixed point (specifically, a saddle-node point):
      </p>
      <ul>
        <li>If \(\tilde{v} < 0\) (meaning \(v < 0.2\)): \(\dot{\tilde{v}}\) is positive. The flow moves <strong>up</strong> towards the fixed point. (Attracting from below).</li>
        <li>If \(\tilde{v} > 0\) (meaning \(v > 0.2\)): \(\dot{\tilde{v}}\) is positive. The flow moves <strong>up</strong> away from the fixed point. (Repelling from above).</li>
      </ul>
      <p>
        <strong>Physical Interpretation:</strong> This explains the "threshold" behavior observed in the simulation.
        Populations below 0.2 recover up to this floor, but once they cross it (due to noise or diffusion), they are pushed rapidly away toward the oscillating limit cycle.
      </p>
        
      <h3>Bifurcations</h3>
      <p>The system has nine parameters, and therefore trying to analyze every interaction is complicated. 
        However, I have observed some bifurcations, especially when varying \(\gamma\) and \(\beta\).
        As \(\gamma\) increases, the limit cycle shrinks and eventually becomes a stable spiral.
        This is expected, since \(\gamma\) controls the recovery rate of the environment/prey population.
        Holding all the other parameters constant, the limit cycle is destroyed around 
        \(\gamma \approx 0.8\), where the interior equilibrium becomes a <strong>stable spiral</strong>,
        whereas before it was <em>unstable</em>. This is a <strong>supercritical Hopf bifurcation</strong>,
        since the limit cycle that is created/destroyed is <strong>stable</strong>. You can experiment with this directly above, 
        try \(\gamma = 0.79\), and you should still see a low amplitude limit cycle. Then try 
        \(\gamma = 0.80\), and you should see trajectories spiral into a point. </p>
        <p>For the parameters used so far, we see periodic waves that radiate outwards from a source. This can be 
          seen in the video below. This setup can produce some interesting features, such as traveling waves, 
          spatial memory, and hysteresis.
        </p>
      <video width="100%" controls autoplay loop muted>
        <source src="Normal Params.mp4" type="video/mp4">
      </video>

        <p>
        The following set of parameters produces a system with Turing-like spatial organization,
         with the periodic pulses reminscent of excitable media.
        The rest of this analysis will be concerned with these parameters, as they produce more interesting 
        behaviors. A video of this can be seen below.
      </p>
      <div class="math-block">\[
        \begin{align}
            r &=10\\
            c &= 4\\
            k_1 &= 12\\
            k_2&= 12\\
            v_c &= 0.4\\
            v_k &= 0.55\\
            \alpha &= 20 \\
            \gamma &= 0.8\\
            \beta &= 3.6
      \end{align}
    \]</div>
    <p></p>
      <video width="100%" autoplay loop muted>
        <source src="Turing-vid.mp4" type="video/mp4">
      </video>

      <p>It behaves similar to a living/breathing network. It also 
        resembles a neuronal network with action potentials propogating along 
        the neuron's axons. The pulses can be understood as 
        temporary surges of local populations. Note that this is only a view of the 
        consumer field, the producer field behaves similarly though.
      </p>
        <h2>Steady-State Analysis</h2>
        <p>
          To understand the long-term behavior of the system, we look for <strong>steady states</strong>—configurations where the 
          concentrations do not change with time (\(\partial u/\partial t = \partial v/\partial t = 0\)). 
          These states satisfy the system of elliptic PDEs:
        </p>

        <div class="math-block">
          \[
          \begin{aligned}
            0 &= D_u \Delta u + F(u,v), \\
            0 &= D_v \Delta v + G(u,v).
          \end{aligned}
          \]
        </div>

        <p>
          This represents a balance where the dispersive effects of diffusion are exactly counteracted by the local reaction kinetics.
        </p>

        <h3>1. Boundary Conditions</h3>
        <p>
          For this system to be well-posed, we must specify boundary conditions. 
          Biologically, we assume the system is isolated, meaning no biomass flows in or out of the domain edges. 
          This corresponds to <strong>homogeneous Neumann boundary conditions</strong>:
        </p>
        <div class="math-block">
          \[ \nabla u \cdot \mathbf{n} = 0, \quad \nabla v \cdot \mathbf{n} = 0 \quad \text{on } \partial \Omega \]
        </div>
        <p>
          where \(\mathbf{n}\) is the outward normal vector. 
          Note that any spatially homogeneous fixed point \((u^*, v^*)\) derived in the ODE section satisfies both the elliptic equations 
          (since \(\Delta u^* = 0\)) and the boundary conditions.
        </p>

        <h3>2. 1D Spatial Dynamics</h3>
        <p>
          In a one-dimensional domain, the steady-state system reduces to a set of second-order ODEs in space. 
          Rearranging terms, we get:
        </p>

        <div class="math-block">
          \[
          \begin{aligned}
            \frac{d^2 u}{dx^2} &= -\frac{1}{D_u} F(u,v), \\
            \frac{d^2 v}{dx^2} &= -\frac{1}{D_v} G(u,v).
          \end{aligned}
          \]
        </div>

        <p>
          This can be viewed as a 4-dimensional dynamical system where the spatial coordinate \(x\) plays the role of "time." 
          While the constant solutions \((u^*, v^*)\) are trivial solutions to this system, we are interested in 
          <strong>non-trivial spatially heterogeneous solutions</strong> (patterns). 
          Finding these usually requires numerical techniques (such as Shooting Methods or Newton-Raphson iteration on the discretized Laplacian), 
          as analytical solutions for coupled nonlinear elliptic systems are rarely possible.
        </p>


<h2>Existence and Uniqueness</h2>
<p>
  To ensure our system is physically meaningful, we must verify that it is "well-posed." 
  Specifically, we establish that for valid initial conditions, a unique solution exists, 
  and that the concentrations remain non-negative and bounded for all time \(t > 0\).
</p>

<div class="claim-box">
  <div class="claim-title">Theorem: Global Existence and Boundedness</div>
  <p>
    <strong>Statement:</strong> Let \(\Omega\) be a bounded domain with smooth boundary and homogeneous Neumann boundary conditions. 
    Suppose the initial data \((u_0, v_0)\) are non-negative and bounded.
    Then, there exists a unique global solution \((u(x,t), v(x,t))\) for all \(t > 0\), and the solution remains uniformly bounded.
  </p>

  <div class="proof-title">Proof Argument.</div>
  
  <p>
    <strong>1. Local Existence and Uniqueness:</strong> 
    First, we observe that the reaction kinetics are continuously differentiable (\(C^1\)) functions of \(u\) and \(v\). 
    Since the domain of interest is bounded, the reaction terms are <strong>locally Lipschitz continuous</strong>.
    By the standard theory for semilinear parabolic equations, this guarantees the existence of a unique local solution on a maximal time interval \([0, T_{\max})\).
  </p>

  <p>
    <strong>2. Positivity (Lower Bounds):</strong> 
    We verify that non-negative initial conditions lead to non-negative solutions (biological viability).
  </p>
  <ul>
    <li>
      <strong>At \(u=0\):</strong> The reaction term is strictly zero (\(f(0,v) = 0\)). Thus \(\partial u / \partial t = 0\), and \(u\) cannot become negative.
    </li>
    <li>
      <strong>At \(v=0\):</strong> The reaction term simplifies to \(\gamma > 0\). The vector field points strictly inward, preventing \(v\) from crossing below zero.
    </li>
  </ul>

  <p>
    <strong>3. Global Boundedness (Prevention of Blow-up):</strong> 
    To prove global existence (\(T_{\max} = \infty\)), we must show the solution does not diverge to infinity in finite time.
  </p>
  <ul>
    <li>
      <strong>Bound for \(u\):</strong> At \(u=1\), the logistic term vanishes, and the decay term dominates:
      \[ \frac{\partial u}{\partial t} = - \frac{cu}{1+e^{-k_2(v_k-v)}} < 0 \]
      Thus, \(u\) is strictly bounded by \(u \le 1\) for all time.
    </li>
    <li>
      <strong>Bound for \(v\):</strong> We do not strictly enforce \(v \le 1\), but rather show \(v\) is bounded by some large constant \(M\). 
      For sufficiently large \(v\), the cubic term \(-\alpha v^3\) dominates the reaction kinetics. 
      Since \(u \ge 0\), the term \(-\beta u v \le 0\) acts as a sink. 
      Therefore, for very large \(v\), \(\frac{\partial v}{\partial t} < 0\). 
      This implies there exists a large invariant rectangle \([0,1] \times [0, M]\) from which the solution cannot escape.
    </li>
  </ul>

  <p>
    <strong>Conclusion:</strong> 
    Since the solution is confined within a bounded region (invariant rectangle) and cannot blow up, it can be extended for all time. 
    Therefore, a unique global solution exists.
  </p>
</div>

        <h2>Linear Stability & Fourier Analysis</h2>
        <p>
            We now investigate the stability of a spatially homogeneous equilibrium \((u^*, v^*)\).
            While the Fourier Transform cannot be easily applied to the full nonlinear system, we can apply it to the 
            <strong>linearized system</strong> to determine which spatial frequencies (wavenumbers) are unstable.
        </p>

        <h3>1. Linearization</h3>
        <p>
            We introduce small perturbations of the form 
            \(u = u^* + \varepsilon \hat{u}\) and \(v = v^* + \varepsilon \hat{v}\).
            Substituting these into the PDE and keeping only first-order terms in \(\varepsilon\), we obtain the linear system:
        </p>

        <div class="math-block">
        \[
        \begin{pmatrix} \hat{u}_t \\ \hat{v}_t \end{pmatrix} 
        = 
        \begin{pmatrix} D_u & 0 \\ 0 & D_v \end{pmatrix} \nabla^2 \begin{pmatrix} \hat{u} \\ \hat{v} \end{pmatrix} 
        + J \begin{pmatrix} \hat{u} \\ \hat{v} \end{pmatrix}
        \]
        </div>

        <p>
            Where \(J\) is the Jacobian matrix evaluated at the fixed point \((u^*, v^*)\).
        </p>

        <h3>2. Spectral Analysis (Dispersion Relation)</h3>
        <p>
            We assume solutions of the form \( \mathbf{w} \sim e^{\lambda t} e^{ikx} \). 
            Substituting this ansatz into the linearized PDE is equivalent to taking the Fourier Transform, 
            where \(\nabla^2 \to -k^2\). This yields the characteristic equation:
        </p>

        <div class="math-block">
        \[ \det(J - k^2 D - \lambda I) = 0 \]
        </div>

        <p>
            Expanding this determinant gives the <strong>Dispersion Relation</strong>, which relates the temporal growth rate \(\lambda\) 
            to the spatial wavenumber \(k\):
        </p>

        <div class="math-block">
        \[ \lambda^2 - \text{tr}(M_k)\lambda + \det(M_k) = 0 \]
        </div>

        <p>
            Where \(M_k = J - k^2 D\). The eigenvalues are given by:
        </p>

        <div class="math-block">
        \[ \lambda_{1,2}(k) = \frac{\text{tr}(J) - k^2(D_u + D_v) \pm \sqrt{(\text{tr}(J) - k^2(D_u+D_v))^2 - 4\det(M_k)}}{2} \]
        </div>

        <h3>3. Turing Instability Conditions</h3>
        <p>
            Turing patterns require four conditions:
        </p>

        <ol>
            <li>
                <strong>ODE Stability:</strong> The homogeneous state must be stable in the absence of diffusion.
                \[ \text{tr}(J) < 0 \quad \text{and} \quad \det(J) > 0 \]
            </li>
            <li>
                <strong>Differential Diffusion:</strong> The inhibitor must diffuse faster than the activator.
                \[ D_v \neq D_u \]
            </li>
            <li>
                <strong>Destabilization:</strong> Diffusion must be strong enough to destabilize the trace or determinant.
                \[ D_v J_{11} + D_u J_{22} > 0 \]
            </li>
            <li>
                <strong>Critical Wavenumber:</strong> The minimum of the determinant \(\det(M_k)\) must be negative, creating a band of unstable modes.
                \[ (D_v J_{11} + D_u J_{22})^2 > 4 D_u D_v \det(J) \]
            </li>
        </ol>
        
        <p>
            If these inequalities hold, the system will form static spots or stripes.
        </p>

        <h3>Verification: Is this a Turing Pattern?</h3>
        <p>
          We can now apply these conditions to the parameters analyzed in the "Stability Analysis" section 
          (\(r=10, \alpha=20, \gamma=0.8, \beta=3.6\)) to see if the observed patterns are strictly "Turing" patterns.
        </p>

        <div class="claim-box">
          <div class="claim-title">Check: Condition 1 (Homogeneous Stability)</div>
          <p>
            <strong>Requirement:</strong> For a Turing instability, the spatially homogeneous fixed point must be stable in the absence of diffusion.
            Mathematically, this requires the real part of the Jacobian eigenvalues to be negative: 
            \(\text{Re}(\lambda) < 0\).
          </p>
          <p>
            <strong>Observation:</strong> In our earlier ODE analysis for the fixed point \((0.345, 0.406)\), we calculated the eigenvalues:
            \[ \lambda \approx 0.507 \pm 2.78 i \]
          </p>
          <p>
            <strong>Conclusion:</strong> Since \(\text{Re}(\lambda) \approx 0.507 > 0\), the homogeneous system is <strong>already unstable</strong> before diffusion is even added. 
            Therefore, <strong>Condition 1 is violated.</strong>
          </p>
        </div>

        <p>
          <strong>Interpretation:</strong> 
          Because the real part is positive and the eigenvalues are complex, the system is undergoing a <strong>Hopf Bifurcation</strong>, not a Turing Bifurcation.
          The patterns appearing in the simulation are not stationary Turing patterns (frozen spots), but rather <strong>traveling waves</strong> or 
          <strong>spatiotemporal chaos</strong> driven by the local oscillatory dynamics of the reaction terms.
          While visually similar to Turing patterns, these are dynamically distinct, resembling "excitable media" (like cardiac tissue or forest fires) 
          rather than stationary morphogensis.
        </p>



    <h2>TO DO:</h2>
    <p>
      <ol>
        <li>Bifurcations in detail</li>
        <li>Amplitude equations</li>
      </ol>
    </p>



  </main>
  <script>
    let canvasVisible = true;
        const params = {
        r: 10,
        c: 4,
        k1: 12,
        k2: 12,
        vc: 0.4,
        vk: 0.55,
        alpha: 12,
        gamma: 0.6,  
        beta: 6      
      };
      const dt = 0.0075;

      // ===================== ODE SYSTEM =====================
      function system(u, v) {
        const { r, c, k1, k2, vc, vk, alpha, gamma, beta } = params;

        const G1 = 1 / (1 + Math.exp(-k1 * (v - vc)));
        const G2 = 1 / (1 + Math.exp(-k2 * (vk - v)));

        const du = r * u * (1 - u) * G1 - c * u * G2;
        const dv = alpha * v * (1 - v) * (v - vc) + gamma * (1 - v) - beta * u * v;

        return [du, dv];
      }

      function step(u, v) {
        const [du, dv] = system(u, v);
        let un = u + dt * du;
        let vn = v + dt * dv;

        return [
          Math.min(Math.max(un, 0), 1),
          Math.min(Math.max(vn, 0), 1)
        ];
      }

      // ===================== CANVAS SETUP =====================
      const canvas = document.getElementById("phaseCanvas");
      const ctx = canvas.getContext("2d");

      let particles = [];
      const observer = new IntersectionObserver((entries) => {
        for (const entry of entries) {
          canvasVisible = entry.isIntersecting;
        }
      });

      observer.observe(canvas);


      function brightColor() {
        const hue = Math.floor(Math.random() * 360);
        return `hsl(${hue}, 95%, 60%)`;
      }

      canvas.addEventListener("click", (e) => {
        const rect = canvas.getBoundingClientRect();
        const u = (e.clientX - rect.left) / rect.width;
        const v = 1 - (e.clientY - rect.top) / rect.height;

        particles.push({
          u,
          v,
          color: brightColor(),
          path: [{ u, v }]
        });
      });

      // ===================== VECTOR FIELD =====================
      function drawVectorField() {
        const spacing = 25;
        ctx.strokeStyle = "rgba(255,255,255,0.08)";
        ctx.lineWidth = 1;

        for (let px = 0; px < canvas.width; px += spacing) {
          for (let py = 0; py < canvas.height; py += spacing) {
            const u = px / canvas.width;
            const v = 1 - py / canvas.height;

            const [du, dv] = system(u, v);

            const scale = 12;
            const x2 = px + scale * du;
            const y2 = py - scale * dv;

            ctx.beginPath();
            ctx.moveTo(px, py);
            ctx.lineTo(x2, y2);
            ctx.stroke();
          }
        }
      }

      // ===================== SLIDER LISTENERS =====================
      const gammaSlider = document.getElementById("gammaSlider");
      const betaSlider = document.getElementById("betaSlider");
      const alphaSlider = document.getElementById("alphaSlider");


      const gammaValue = document.getElementById("gammaValue");
      const betaValue = document.getElementById("betaValue");
      const alphaValue = document.getElementById("alphaValue");


      // Update text while dragging
      gammaSlider.addEventListener("input", (e) => {
        gammaValue.textContent = parseFloat(e.target.value).toFixed(2);
      });

      betaSlider.addEventListener("input", (e) => {
        betaValue.textContent = parseFloat(e.target.value).toFixed(2);
      });
      alphaSlider.addEventListener("input", (e) => {
        alphaValue.textContent = parseFloat(e.target.value).toFixed(2);
      });

      gammaSlider.addEventListener("change", (e) => {
        params.gamma = parseFloat(e.target.value);
        particles = [];
      });

      betaSlider.addEventListener("change", (e) => {
        params.beta = parseFloat(e.target.value);
        particles = [];
      });
      alphaSlider.addEventListener("change", (e) => {
        params.alpha = parseFloat(e.target.value);
        particles = [];
      });
      document.getElementById("resetBtn").addEventListener("click", () => {
          particles = [];
        });
        document.getElementById("resetParamsBtn").addEventListener("click", () => {
            // Restore defaults
            params.gamma = 0.6;
            params.beta = 6;
            params.alpha = 12;

            // Update sliders
            gammaSlider.value = params.gamma;
            betaSlider.value = params.beta;
            alphaSlider.value = params.alpha;

            // Update display values
            gammaValue.textContent = params.gamma.toFixed(2);
            betaValue.textContent = params.beta.toFixed(2);
            alphaValue.textContent = params.alpha.toFixed(2);

            // Optional: clear trajectories after param change
            particles.length = 0;
          });



      // ===================== MAIN LOOP =====================
      function draw() {
        if (!canvasVisible) {
          requestAnimationFrame(draw);
          return;
        }

        ctx.fillStyle = "#0f1115";
        ctx.fillRect(0, 0, canvas.width, canvas.height);

        drawVectorField();

        for (let p of particles) {
          const [nu, nv] = step(p.u, p.v);
          p.u = nu;
          p.v = nv;
          p.path.push({ u: nu, v: nv });

          ctx.strokeStyle = p.color;
          ctx.lineWidth = 2;
          ctx.beginPath();
          for (let i = 0; i < p.path.length - 1; i++) {
            const a = p.path[i];
            const b = p.path[i + 1];
            ctx.moveTo(a.u * canvas.width, (1 - a.v) * canvas.height);
            ctx.lineTo(b.u * canvas.width, (1 - b.v) * canvas.height);
          }
          ctx.stroke();

          ctx.fillStyle = p.color;
          ctx.beginPath();
          ctx.arc(
            p.u * canvas.width,
            (1 - p.v) * canvas.height,
            4.5,
            0,
            2 * Math.PI
          );
          ctx.fill();
        }

        requestAnimationFrame(draw);
      }
      draw();
</script>

</body>
</html>
